{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMXhY2n5CkLQbx+uUiqz0Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## N-gram model\n","\n","This is to train a bigram and trigram language model."],"metadata":{"id":"9CZnEWd6F0Sx"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","\n","\n","import sys, re\n","import numpy as np\n","import math\n"],"metadata":{"id":"a67YplacP4mW","executionInfo":{"status":"ok","timestamp":1670877919146,"user_tz":300,"elapsed":5888,"user":{"displayName":"Shashank Holla","userId":"01861841876262582364"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["\n","\n","###############################################################################\n","\n","def preprocess(s):\n","    \"\"\"Tokenise a line\"\"\"\n","    o = re.sub('([^a-zA-Z0-9\\']+)', ' \\g<1> ', s.strip())\n","    return ['<BOS>'] + re.sub('  *', ' ', o).strip().split(' ')\n","\n","###############################################################################"],"metadata":{"id":"ZvumnvtIQBOm","executionInfo":{"status":"ok","timestamp":1670877919148,"user_tz":300,"elapsed":8,"user":{"displayName":"Shashank Holla","userId":"01861841876262582364"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["training_samples = []\n","vocabulary = set(['<UNK>'])\n","\n","def file_read(filename):\n","    with open(filename) as f:\n","        lines = [line.rstrip() for line in f]\n","        return lines\n","\n","lines = file_read(\"train.txt\")\n","\n","for line in lines:\n","    tokens = preprocess(line)\n","    for i in tokens: vocabulary.add(i) \n","    training_samples.append(tokens)\n","\n","\n","word2idx = {k: v for v, k in enumerate(vocabulary)}\n","idx2word = {v: k for k, v in word2idx.items()}\n","\n","x_train = []\n","y_train = []\n","for tokens in training_samples:\n","    for i in range(len(tokens) - 2): #!!!#\n","        x_train.append([word2idx[tokens[i]], word2idx[tokens[i+1]]]) #!!!#\n","        y_train.append([word2idx[tokens[i+2]]]) #!!!#\n","\n","x_train = np.array(x_train)\n","y_train = np.array(y_train)\n","\n","###############################################################################\n","\n","BATCH_SIZE = 1\n","NUM_EPOCHS = 10\n","\n","train_set = np.concatenate((x_train, y_train), axis=1)"],"metadata":{"id":"Y1w33KmdRbjg","executionInfo":{"status":"ok","timestamp":1670877977835,"user_tz":300,"elapsed":141,"user":{"displayName":"Shashank Holla","userId":"01861841876262582364"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["train_set"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PFf6cS6VYfnD","executionInfo":{"status":"ok","timestamp":1670878757102,"user_tz":300,"elapsed":143,"user":{"displayName":"Shashank Holla","userId":"01861841876262582364"}},"outputId":"32741af0-c152-4ddf-8b49-f0193d7ae98e"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[13, 15,  2],\n","       [15,  2,  3],\n","       [ 2,  3,  1],\n","       [ 3,  1,  9],\n","       [13,  8, 15],\n","       [ 8, 15,  2],\n","       [15,  2,  9],\n","       [13, 15,  2],\n","       [15,  2,  5],\n","       [ 2,  5,  9],\n","       [13,  6, 12],\n","       [ 6, 12,  5],\n","       [12,  5, 10],\n","       [13, 15,  2],\n","       [15,  2, 14],\n","       [ 2, 14,  4],\n","       [14,  4,  9],\n","       [13,  0,  2],\n","       [ 0,  2, 14],\n","       [ 2, 14, 11],\n","       [14, 11,  9]])"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","execution_count":29,"metadata":{"id":"XnIVkUD6OetE","executionInfo":{"status":"ok","timestamp":1670878774089,"user_tz":300,"elapsed":321,"user":{"displayName":"Shashank Holla","userId":"01861841876262582364"}}},"outputs":[],"source":["# Bigram Neural Network Model\n","class BigramNNmodel(nn.Module):\n","\n","    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n","        super(BigramNNmodel, self).__init__()\n","        self.context_size = context_size\n","        self.embedding_dim = embedding_dim\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n","        self.linear2 = nn.Linear(hidden_dim, vocab_size, bias = False)\n","\n","    def forward(self, inputs):\n","        # compute x': concatenation of x1 and x2 embeddings\n","        embeds = self.embeddings(inputs).view(\n","                (-1,self.context_size * self.embedding_dim))\n","        # compute h: tanh(W_1.x' + b)\n","        out = torch.tanh(self.linear1(embeds))\n","        # compute W_2.h\n","        out = self.linear2(out)\n","        # compute y: log_softmax(W_2.h)\n","        log_probs = F.log_softmax(out, dim=1)\n","        # return log probabilities\n","        # BATCH_SIZE x len(vocab)\n","        return log_probs"]},{"cell_type":"markdown","source":["### Training and testing bigram model"],"metadata":{"id":"f1wNETbkGjiq"}},{"cell_type":"code","source":["EMBEDDING_DIM = 4\n","CONTEXT_SIZE = 1 #!!!#\n","HIDDEN_DIM = 6"],"metadata":{"id":"NzYcq31iP-Sx","executionInfo":{"status":"ok","timestamp":1670878776512,"user_tz":300,"elapsed":134,"user":{"displayName":"Shashank Holla","userId":"01861841876262582364"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["def train(train_file):        \n","    import sys, re\n","    import numpy as np\n","    import math\n","\n","    ###############################################################################\n","\n","    training_samples = []\n","    vocabulary = set(['<UNK>'])\n","    lines = file_read(train_file)\n","\n","    # line = sys.stdin.readline()\n","    # while line:\n","    #     tokens = preprocess(line)\n","    #     for i in tokens: vocabulary.add(i) \n","    #     training_samples.append(tokens)\n","    #     line = sys.stdin.readline()\n","\n","    for line in lines:\n","        tokens = preprocess(line)\n","        for i in tokens: vocabulary.add(i) \n","        training_samples.append(tokens)\n","\n","\n","    word2idx = {k: v for v, k in enumerate(vocabulary)}\n","    idx2word = {v: k for k, v in word2idx.items()}\n","\n","    x_train = []\n","    y_train = []\n","    for tokens in training_samples:\n","        for i in range(len(tokens) - 1): #!!!#\n","            x_train.append([word2idx[tokens[i]]]) #!!!#\n","            y_train.append([word2idx[tokens[i+1]]]) #!!!#\n","\n","    x_train = np.array(x_train)\n","    y_train = np.array(y_train)\n","\n","    ###############################################################################\n","\n","    BATCH_SIZE = 1\n","    NUM_EPOCHS = 10\n","\n","    train_set = np.concatenate((x_train, y_train), axis=1)\n","    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n","\n","    loss_function = nn.NLLLoss()\n","    model = BigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n","    optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","    for epoch in range(NUM_EPOCHS):\n","        for i, data_tensor in enumerate(train_loader):\n","            context_tensor = data_tensor[:,0:1] #!!!#\n","            target_tensor = data_tensor[:,1] #!!!#\n","\n","            model.zero_grad()\n","\n","            log_probs = model(context_tensor)\n","            loss = loss_function(log_probs, target_tensor)\n","\n","            loss.backward()\n","            optimiser.step()    \n","\n","        print('Epoch:', epoch, 'loss:', float(loss))\n","\n","    torch.save({'model': model.state_dict(), 'vocab': idx2word}, 'model_bigram.lm')\n","\n","    print('Model saved.')"],"metadata":{"id":"rWkFigw9G4rN","executionInfo":{"status":"ok","timestamp":1670879019822,"user_tz":300,"elapsed":114,"user":{"displayName":"Shashank Holla","userId":"01861841876262582364"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["train(\"train.txt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3RIkLwsDHJkV","executionInfo":{"status":"ok","timestamp":1670879022457,"user_tz":300,"elapsed":457,"user":{"displayName":"Shashank Holla","userId":"01861841876262582364"}},"outputId":"86849e55-3fcc-4e08-c4d3-193f6d4ab1ef"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0 loss: 2.890814781188965\n","Epoch: 1 loss: 2.7067513465881348\n","Epoch: 2 loss: 2.524578809738159\n","Epoch: 3 loss: 2.319181442260742\n","Epoch: 4 loss: 2.0889270305633545\n","Epoch: 5 loss: 1.8314764499664307\n","Epoch: 6 loss: 1.5535516738891602\n","Epoch: 7 loss: 1.2740225791931152\n","Epoch: 8 loss: 1.0172144174575806\n","Epoch: 9 loss: 0.8042901158332825\n","Model saved.\n"]}]},{"cell_type":"code","source":["def test(test_file):\n","    blob = torch.load('model_bigram.lm')\n","    idx2word = blob['vocab']\n","    word2idx = {k: v for v, k in idx2word.items()}\n","    vocabulary = set(idx2word.values())\n","\n","    model = BigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n","    model.load_state_dict(blob['model'])\n","\n","    ###############################################################################\n","\n","    BATCH_SIZE = 1\n","\n","    lines = file_read(test_file)\n","    \n","    for line in lines:\n","        tokens = preprocess(line)\n","        \n","        x_test = []\n","        y_test = []\n","        for i in range(len(tokens) - 1): #!!!#\n","            x_test.append([word2idx[tokens[i]]]) #!!!#\n","            y_test.append([word2idx[tokens[i+1]]]) #!!!#\n","        \n","        x_test = np.array(x_test)\n","        y_test = np.array(y_test)\n","        \n","        test_set = np.concatenate((x_test, y_test), axis=1)\n","        test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n","        \n","        total_prob = 1.0\n","        for i, data_tensor in enumerate(test_loader):\n","            context_tensor = data_tensor[:,0:1] #!!!#\n","            target_tensor = data_tensor[:,1] #!!!#\n","            log_probs = model(context_tensor)\n","            probs = torch.exp(log_probs)\n","            predicted_label = int(torch.argmax(probs, dim=1)[0])\n","        \n","            true_label = y_test[i][0]\n","            true_word = idx2word[true_label]\n","        \n","            prob_true = float(probs[0][true_label])\n","            total_prob *= prob_true\n","        \n","        print('%.6f\\t%.6f\\t' % (total_prob, math.log(total_prob)), tokens)"],"metadata":{"id":"TXHkA6U0JC99","executionInfo":{"status":"ok","timestamp":1670879060631,"user_tz":300,"elapsed":166,"user":{"displayName":"Shashank Holla","userId":"01861841876262582364"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["test(\"test.txt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hbY21sZMJ0ew","executionInfo":{"status":"ok","timestamp":1670879071885,"user_tz":300,"elapsed":127,"user":{"displayName":"Shashank Holla","userId":"01861841876262582364"}},"outputId":"0c3c8b50-1aef-486e-92ac-2e61ffcaa293"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["0.005596\t-5.185768\t ['<BOS>', 'where', 'are', 'you', '?']\n","0.000946\t-6.962973\t ['<BOS>', 'were', 'you', 'in', 'england', '?']\n","0.011601\t-4.456650\t ['<BOS>', 'are', 'you', 'in', 'mexico', '?']\n","0.000014\t-11.157101\t ['<BOS>', 'i', 'am', 'in', 'mexico', '.']\n","0.000124\t-8.995025\t ['<BOS>', 'are', 'you', 'still', 'in', 'mexico', '?']\n"]}]},{"cell_type":"markdown","source":["### Training and testing a trigram model"],"metadata":{"id":"UKz8waING1Oh"}},{"cell_type":"code","source":["EMBEDDING_DIM = 4\n","CONTEXT_SIZE = 2 #!!!#\n","HIDDEN_DIM = 6"],"metadata":{"id":"vBf5cm-eJ5Oq","executionInfo":{"status":"ok","timestamp":1670879119928,"user_tz":300,"elapsed":162,"user":{"displayName":"Shashank Holla","userId":"01861841876262582364"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# Trigram Neural Network Model\n","class TrigramNNmodel(nn.Module):\n","\n","    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n","        super(TrigramNNmodel, self).__init__()\n","        self.context_size = context_size\n","        self.embedding_dim = embedding_dim\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n","        self.linear2 = nn.Linear(hidden_dim, vocab_size, bias = False)\n","\n","    def forward(self, inputs):\n","        # compute x': concatenation of x1 and x2 embeddings\n","        embeds = self.embeddings(inputs).view(\n","                (-1,self.context_size * self.embedding_dim))\n","        # compute h: tanh(W_1.x' + b)\n","        out = torch.tanh(self.linear1(embeds))\n","        # compute W_2.h\n","        out = self.linear2(out)\n","        # compute y: log_softmax(W_2.h)\n","        log_probs = F.log_softmax(out, dim=1)\n","        # return log probabilities\n","        # BATCH_SIZE x len(vocab)\n","        return log_probs"],"metadata":{"id":"kkcddMyVKAxQ","executionInfo":{"status":"ok","timestamp":1670879136960,"user_tz":300,"elapsed":445,"user":{"displayName":"Shashank Holla","userId":"01861841876262582364"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["def train(train_file):        \n","    import sys, re\n","    import numpy as np\n","    import math\n","\n","    ###############################################################################\n","\n","    training_samples = []\n","    vocabulary = set(['<UNK>'])\n","    lines = file_read(train_file)\n","\n","    # line = sys.stdin.readline()\n","    # while line:\n","    #     tokens = preprocess(line)\n","    #     for i in tokens: vocabulary.add(i) \n","    #     training_samples.append(tokens)\n","    #     line = sys.stdin.readline()\n","\n","    for line in lines:\n","        tokens = preprocess(line)\n","        for i in tokens: vocabulary.add(i) \n","        training_samples.append(tokens)\n","\n","\n","    word2idx = {k: v for v, k in enumerate(vocabulary)}\n","    idx2word = {v: k for k, v in word2idx.items()}\n","\n","    x_train = []\n","    y_train = []\n","    for tokens in training_samples:\n","        for i in range(len(tokens) - 2): #!!!#\n","            x_train.append([word2idx[tokens[i]], word2idx[tokens[i+1]]]) #!!!#\n","            y_train.append([word2idx[tokens[i+2]]]) #!!!#\n","\n","    x_train = np.array(x_train)\n","    y_train = np.array(y_train)\n","\n","    ###############################################################################\n","\n","    BATCH_SIZE = 1\n","    NUM_EPOCHS = 10\n","\n","    train_set = np.concatenate((x_train, y_train), axis=1)\n","    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n","\n","    loss_function = nn.NLLLoss()\n","    model = TrigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n","    optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","    for epoch in range(NUM_EPOCHS):\n","        for i, data_tensor in enumerate(train_loader):\n","            context_tensor = data_tensor[:,0:2] #!!!#\n","            target_tensor = data_tensor[:,2] #!!!#\n","\n","            model.zero_grad()\n","\n","            log_probs = model(context_tensor)\n","            loss = loss_function(log_probs, target_tensor)\n","\n","            loss.backward()\n","            optimiser.step()    \n","\n","        print('Epoch:', epoch, 'loss:', float(loss))\n","\n","    torch.save({'model': model.state_dict(), 'vocab': idx2word}, 'model_trigram.lm')\n","\n","    print('Model saved.')"],"metadata":{"id":"OZsEefUGQPCG","executionInfo":{"status":"ok","timestamp":1670879165522,"user_tz":300,"elapsed":118,"user":{"displayName":"Shashank Holla","userId":"01861841876262582364"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["train(\"train.txt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y0XY78LBV22N","executionInfo":{"status":"ok","timestamp":1670879168159,"user_tz":300,"elapsed":247,"user":{"displayName":"Shashank Holla","userId":"01861841876262582364"}},"outputId":"12843e8c-0448-4970-e559-3d3a068d185c"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0 loss: 2.7683162689208984\n","Epoch: 1 loss: 2.051835536956787\n","Epoch: 2 loss: 1.4063498973846436\n","Epoch: 3 loss: 0.9947658181190491\n","Epoch: 4 loss: 0.7658755779266357\n","Epoch: 5 loss: 0.6151495575904846\n","Epoch: 6 loss: 0.5007540583610535\n","Epoch: 7 loss: 0.4108475148677826\n","Epoch: 8 loss: 0.3393906354904175\n","Epoch: 9 loss: 0.2827325463294983\n","Model saved.\n"]}]},{"cell_type":"code","source":["def test(test_file):\n","    blob = torch.load('model_trigram.lm')\n","    idx2word = blob['vocab']\n","    word2idx = {k: v for v, k in idx2word.items()}\n","    vocabulary = set(idx2word.values())\n","\n","    model = BigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n","    model.load_state_dict(blob['model'])\n","\n","    ###############################################################################\n","\n","    BATCH_SIZE = 1\n","\n","    lines = file_read(test_file)\n","    \n","    for line in lines:\n","        tokens = preprocess(line)\n","        \n","        x_test = []\n","        y_test = []\n","        for i in range(len(tokens) - 2): #!!!#\n","            x_test.append([word2idx[tokens[i]], word2idx[tokens[i+1]]]) #!!!#\n","            y_test.append([word2idx[tokens[i+2]]]) #!!!#\n","        \n","        x_test = np.array(x_test)\n","        y_test = np.array(y_test)\n","        \n","        test_set = np.concatenate((x_test, y_test), axis=1)\n","        test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n","        \n","        total_prob = 1.0\n","        for i, data_tensor in enumerate(test_loader):\n","            context_tensor = data_tensor[:,0:2] #!!!#\n","            target_tensor = data_tensor[:,2] #!!!#\n","            log_probs = model(context_tensor)\n","            probs = torch.exp(log_probs)\n","            predicted_label = int(torch.argmax(probs, dim=1)[0])\n","        \n","            true_label = y_test[i][0]\n","            true_word = idx2word[true_label]\n","        \n","            prob_true = float(probs[0][true_label])\n","            total_prob *= prob_true\n","        \n","        print('%.6f\\t%.6f\\t' % (total_prob, math.log(total_prob)), tokens)\n","        "],"metadata":{"id":"SqjPgsbvQkxm","executionInfo":{"status":"ok","timestamp":1670879186754,"user_tz":300,"elapsed":136,"user":{"displayName":"Shashank Holla","userId":"01861841876262582364"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["test(\"test.txt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a1WqpPpiVSse","executionInfo":{"status":"ok","timestamp":1670879188606,"user_tz":300,"elapsed":105,"user":{"displayName":"Shashank Holla","userId":"01861841876262582364"}},"outputId":"79404679-5955-4c96-fe4b-5658a562f935"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["0.043800\t-3.128118\t ['<BOS>', 'where', 'are', 'you', '?']\n","0.035767\t-3.330737\t ['<BOS>', 'were', 'you', 'in', 'england', '?']\n","0.038391\t-3.259942\t ['<BOS>', 'are', 'you', 'in', 'mexico', '?']\n","0.000484\t-7.633777\t ['<BOS>', 'i', 'am', 'in', 'mexico', '.']\n","0.000565\t-7.478812\t ['<BOS>', 'are', 'you', 'still', 'in', 'mexico', '?']\n"]}]}]}